{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import operator\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import polyglot\n",
    "from polyglot.downloader import downloader\n",
    "from polyglot.text import Text\n",
    "import wikipedia\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "from polyglot.detect import Detector\n",
    "from datetime import datetime, date\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectors():\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory\n",
    "    \n",
    "    def make_vectors(self, directory):\n",
    "        table = dict()\n",
    "        with open(directory, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            next(f)\n",
    "            vectors = []\n",
    "            for i, line in enumerate(f):\n",
    "                word,vect = line.rstrip().split(\" \",1)\n",
    "                vect = np.fromstring(vect, sep=' ')\n",
    "                table[word] = vect\n",
    "        return table\n",
    "    \n",
    "    def paths (self):\n",
    "        dict_tables = {}\n",
    "        for v in self.directory:\n",
    "            if v == 'wiki.multi.en.vec.txt':\n",
    "                dict_tables['EN'] = self.make_vectors(v)\n",
    "            elif v == 'wiki.multi.es.vec.txt':\n",
    "                dict_tables['ES'] = self.make_vectors(v)\n",
    "            elif v == 'wiki.multi.it.vec.txt':\n",
    "                dict_tables['IT'] = self.make_vectors(v)\n",
    "            elif v == 'wiki.multi.et.vec.txt':\n",
    "                dict_tables['ET'] = self.make_vectors(v)\n",
    "            elif v == 'wiki.multi.ru.vec.txt':\n",
    "                dict_tables['RU'] = self.make_vectors(v)\n",
    "            elif v == 'wiki.multi.de.vec.txt':\n",
    "                dict_tables['DE'] = self.make_vectors(v)\n",
    "\n",
    "        return dict_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Keywords():\n",
    "    def __init__(self, keywords):\n",
    "        self.keywords = keywords\n",
    "\n",
    "    def make_keywords(self, table_lang):\n",
    "        key_dict = dict()\n",
    "        for k in self.keywords:\n",
    "            key_dict[k] = table_lang[k]\n",
    "        return key_dict\n",
    "    \n",
    "    def make_empty_dictionaries(self):\n",
    "        similarities_lang = dict()\n",
    "        for k in self.keywords:\n",
    "            similarities_lang[k] = np.asarray([[0]])\n",
    "        return similarities_lang\n",
    "    \n",
    "    def empty_dictionaries(self, dict_table):\n",
    "        similarities_per_language = dict()\n",
    "        for k,v in dict_table.items():\n",
    "            similarities_per_language[k] = (self.make_empty_dictionaries())\n",
    "        return similarities_per_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeBot():\n",
    "   \n",
    "    # Function that captures the highest similarity within the token and the keywords for a language\n",
    "    def highest_similarity_for_keyword(self, empty_dictionaries, dict_multi_vectors, key, value):\n",
    "        last_status = empty_dictionaries[key]\n",
    "        similarity = cosine_similarity(dict_multi_vectors.reshape(1,300), value)\n",
    "        if similarity > last_status:\n",
    "            empty_dictionaries[key] = similarity\n",
    "            \n",
    "    # Function that compares the similarities between all languages and returns the highest one overall\n",
    "    def highest_sim_over_language(self, empty_dictionaries, dict_multi_vectors, key, value, multibot_dict):\n",
    "    \n",
    "        self.highest_similarity_for_keyword(empty_dictionaries['EN'], dict_multi_vectors['EN'], key, value)\n",
    "        \n",
    "        try:\n",
    "            #Sp\n",
    "            self.highest_similarity_for_keyword(empty_dictionaries['ES'], dict_multi_vectors['ES'], key, value)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            #It\n",
    "            self.highest_similarity_for_keyword(empty_dictionaries['IT'], dict_multi_vectors['IT'], key, value)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            #Rus\n",
    "            self.highest_similarity_for_keyword(empty_dictionaries['RU'], dict_multi_vectors['RU'], key, value)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            #Est\n",
    "            self.highest_similarity_for_keyword(empty_dictionaries['ET'], dict_multi_vectors['ET'], key, value)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            #Est\n",
    "            self.highest_similarity_for_keyword(empty_dictionaries['DE'], dict_multi_vectors['DE'], key, value)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # sort the keys by the highest cosine values of all languages\n",
    "        sorted_results = dict()\n",
    "        for k,v in empty_dictionaries.items():\n",
    "            sorted_results[k] = sorted(v.items(), key=operator.itemgetter(1), reverse=True)\n",
    "            list_key_value = [[k,v] for k, v in sorted_results.items()]\n",
    "            \n",
    "        for items in list_key_value:\n",
    "            lang = items[0]\n",
    "            pairs = items[1]\n",
    "            multibot_dict[pairs[0][0], lang] = pairs[0][1]\n",
    "\n",
    "        sorted_simil = sorted(multibot_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        \n",
    "        return sorted_simil\n",
    "    \n",
    "\n",
    "  \n",
    "    def compute_highest_token(self, bot_dictionary, empty_dictionaries, cutoff,  dict_multi_vectors,\n",
    "                              highest_token_dict, multibot_dict):\n",
    "       \n",
    "        #bot dictionary is the dictionary of the CLWEs for the keywords selected\n",
    "        for key, value in bot_dictionary.items():\n",
    "            value = value.reshape(1,300)\n",
    "            # we keep in the dict the value that had the highest cosine similarity between key and CLWE\n",
    "            sorted_simil = self.highest_sim_over_language(empty_dictionaries, dict_multi_vectors, key, value, multibot_dict)\n",
    "        \n",
    "        language = sorted_simil[0][0][1]\n",
    "        # get the highest key-value pair\n",
    "        if sorted_simil[0][1][0][0] > cutoff:\n",
    "            highest_token_dict[sorted_simil[0][0][0]] = sorted_simil[0][1][0][0] \n",
    "        # check if the highest key-value pair is over the threshold and append for the confidence\n",
    "        # CONFIDENCE IS GIVEN BY THE SUM OF CAPTURED KEYWORDS ABOVE THE THRESHOLD\n",
    "        #if sorted_simil[0][1][0][0] > cutoff:\n",
    "         #   confidence.append(sorted_simil[0][1][0][0] + boost)\n",
    "            \n",
    "        return language\n",
    "    \n",
    "    \n",
    "    def compute_confidence(self, language, bot_dictionary,  dict_multi_vectors, multibot_dict, confidence, cutoff, boost):\n",
    "        # sort the keys by the highest cosine values of all languages\n",
    "        for key, value in bot_dictionary.items():\n",
    "            value = value.reshape(1,300)\n",
    "            similarity = cosine_similarity(dict_multi_vectors[language].reshape(1,300), value)\n",
    "            if similarity[0][0] > cutoff:\n",
    "                confidence.append(similarity[0][0] + boost)\n",
    "\n",
    "                \n",
    "    def token2multi_vectors(self, token, all_vectors):\n",
    "        UNK = np.zeros((1, 300))\n",
    "        dict_vect = dict()\n",
    "        for k,v in all_vectors.items():\n",
    "            try:\n",
    "                dict_vect[k] = v[token].reshape(1,300)\n",
    "            except:\n",
    "                dict_vect[k] = UNK\n",
    "        return dict_vect  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolyBot():\n",
    "    def __init__(self, keywords, dict_CLWEs, kw_lang, answer, cutoff=0.43, boost=0.5, all_lang=None, bigrams=None, \n",
    "                 bigram_cutoff = 0.8, bigram_boost = 1):\n",
    "        self.keywords = keywords\n",
    "        self.dict_CLWEs = dict_CLWEs\n",
    "        self.kw_lang = kw_lang\n",
    "        self.answer = answer\n",
    "        self.bigrams = bigrams\n",
    "        self.cutoff = cutoff\n",
    "        self.boost = boost\n",
    "        self.all_lang = all_lang\n",
    "        self.bigram_cutoff = bigram_cutoff\n",
    "        self.bigram_boost = bigram_boost\n",
    "        self.k = Keywords(self.keywords)\n",
    "        self.m = MakeBot()\n",
    "        \n",
    "    def prepare_bot(self):\n",
    "        empty_dictionaries = self.k.empty_dictionaries(self.dict_CLWEs)\n",
    "        confidence = []\n",
    "        multibot_dict = dict()\n",
    "        highest_token = dict()\n",
    "        return  empty_dictionaries, confidence, multibot_dict, highest_token\n",
    "        \n",
    "    def language_identifier (self, empty_dictionaries, dict_multi_vectors, highest_token_dict, multibot_dict):\n",
    "        bot_dictionary = self.k.make_keywords(self.dict_CLWEs[self.kw_lang]) \n",
    "        \n",
    "        language = self.m.compute_highest_token(bot_dictionary, empty_dictionaries, self.cutoff,\n",
    "                                     dict_multi_vectors, highest_token_dict, multibot_dict)\n",
    "        \n",
    "        return language\n",
    "    \n",
    "    \n",
    "    def get_confidence(self, dict_multivector, confidence, multibot_dict, language):\n",
    "        bot_dictionary = self.k.make_keywords(self.dict_CLWEs[self.kw_lang])\n",
    "        self.m.compute_confidence(language, bot_dictionary, dict_multivector, multibot_dict, confidence, self.cutoff, self.boost)\n",
    "    \n",
    "    def else_language(self, dict_multi_vectors, dict_next_multivector):\n",
    "        source_lang = []\n",
    "        else_lang = []\n",
    "        next_source_lang = []\n",
    "        next_else_lang = []\n",
    "        \n",
    "        for k, v in dict_multi_vectors.items():\n",
    "            if k == self.kw_lang:\n",
    "                source_lang.append(v)\n",
    "            else:\n",
    "                else_lang.append(v)\n",
    "        for k, v in dict_next_multivector.items():\n",
    "            if k == self.kw_lang:\n",
    "                next_source_lang.append(v)\n",
    "            else:\n",
    "                next_else_lang.append(v) \n",
    "        return source_lang, else_lang, next_source_lang, next_else_lang\n",
    "    \n",
    "    def score_bigrams(self, w1,w2, source_lang, else_lang, next_source_lang, next_else_lang, confidence):\n",
    "        for i in range(len(source_lang)):\n",
    "            if cosine_similarity(self.dict_CLWEs[self.kw_lang][str(w1)].reshape(1,300), source_lang[i]) + cosine_similarity(self.dict_CLWEs[self.kw_lang][str(w2)].reshape(1,300), next_source_lang[i]) > 1.6:\n",
    "                confidence.append(self.bigram_boost)\n",
    "                return True\n",
    "        for i in range(len(else_lang)):\n",
    "            if cosine_similarity(self.dict_CLWEs[self.kw_lang][str(w1)].reshape(1,300), else_lang[i]) + cosine_similarity(self.dict_CLWEs[self.kw_lang][str(w2)].reshape(1,300), next_else_lang[i]) > self.bigram_cutoff:\n",
    "                self.confidence.append(self.bigram_boost)\n",
    "                return True\n",
    "    \n",
    "    def compute_bigrams(self, w1, w2, dict_multivector, dict_next_multivector, confidence):\n",
    "        source_lang, else_lang, next_source_lang, next_else_lang = self.else_language(dict_multivector, dict_next_multivector)\n",
    "        w = self.score_bigrams(w1,w2, source_lang, else_lang, next_source_lang, next_else_lang, confidence)\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.m = MakeBot()\n",
    "\n",
    "    # Function to wikify the entity\n",
    "    def selenium (self, url):\n",
    "        chrome_options = Options() \n",
    "        chrome_options.add_argument(\"--headless\") \n",
    "        driver = webdriver.Chrome(r'C:/Users/Jason/Downloads/chromedriver_win32/chromedriver.exe', options=chrome_options)\n",
    "        driver.get(url)\n",
    "        driver.find_element_by_link_text('English').click()\n",
    "        name= driver.title\n",
    "        LANGS = [\n",
    "        'ÁáČčĎďÉéĚěÍíŇňÓóŘřŠšŤťÚúŮůÝýŽž',   # Czech\n",
    "        'ÄäÖöÜüẞß',                         # German\n",
    "        'ĄąĆćĘęŁłŃńÓóŚśŹźŻż',               # Polish\n",
    "        'áéóíñü',                           # Spanish\n",
    "        'àèòìù'                             #Italian\n",
    "        ]\n",
    "        pattern = r'[A-Z][a-z{langs}]+'.format(langs=''.join(LANGS))\n",
    "        pattern = re.compile(pattern)\n",
    "        match = pattern.findall(name)\n",
    "        match = match[0:-1]\n",
    "        try:\n",
    "            if match [1] != None:\n",
    "                return match[0] + \" \" + match[1]\n",
    "        except:\n",
    "            return match[0]\n",
    "    \n",
    "    def prep_city(self, from_phrase, parola, lang, all_vectors):\n",
    "        self.table_lang = all_vectors['EN']\n",
    "        prep = 0\n",
    "        print('BOT:', from_phrase) \n",
    "        city = input('YOU: ')\n",
    "        tok = city.lower().split()\n",
    "        for i in range(len(tok)):\n",
    "            vect_dict = self.m.token2multi_vectors(tok[i], all_vectors)\n",
    "            for k,v in vect_dict.items():\n",
    "                if lang == 'en':\n",
    "                    if cosine_similarity(v, self.table_lang[parola].reshape(1,300)) > 0.8:\n",
    "                        prep += 1\n",
    "                else:\n",
    "                    if cosine_similarity(v, self.table_lang[parola].reshape(1,300)) > 0.3:\n",
    "                        prep += 1       \n",
    "        if prep > 0: \n",
    "            try:\n",
    "                if tok[2] != None:\n",
    "                    CITY = tok[1].capitalize() + \" \" + tok[2].capitalize()      \n",
    "            except:\n",
    "                try:\n",
    "                    CITY = tok[1].capitalize()\n",
    "                except:\n",
    "                    CITY = city.capitalize()\n",
    "        else:\n",
    "            CITY = city.capitalize()\n",
    "            \n",
    "        if lang != 'en':\n",
    "            try:\n",
    "                website = 'https://{}.wikipedia.org/wiki/{}'.format(lang, CITY)\n",
    "                CITY = self.selenium(website)\n",
    "            except:\n",
    "                CITY = translator.translate(CITY, dest=lang).text\n",
    "       \n",
    "        return CITY \n",
    "   \n",
    "    # function that intercepts the vectors related to the time of the day\n",
    "    def when_def(self,when_dict, conf_value, vect_dict):\n",
    "        sim_today = []\n",
    "        for key, value in when_dict.items():\n",
    "            value = value.reshape(1,300)\n",
    "            for k,v in vect_dict.items():\n",
    "                sim = cosine_similarity(v, value)\n",
    "                if sim > conf_value:\n",
    "                    sim_today.append(sim)\n",
    "        return sum(sim_today)\n",
    "    \n",
    "    def find_TodayTomorrow(self, vect_dict, all_vectors):\n",
    "        today = False\n",
    "        today_key = {'today':all_vectors['EN']['today'], 'now':all_vectors['EN']['now']}\n",
    "        oggi = self.when_def(today_key, 0.5, vect_dict)\n",
    "        tomorrow = False\n",
    "        tomorrow_key = {'tomorrow':all_vectors['EN']['tomorrow']}\n",
    "        domani = self.when_def(tomorrow_key, 0.45, vect_dict)\n",
    "        if oggi > 0 and oggi > domani:\n",
    "            today = True\n",
    "        if domani > 0 and domani > oggi:\n",
    "            tomorrow = True\n",
    "        return today, tomorrow\n",
    "    \n",
    "    def NER(self, human):\n",
    "        PER = []\n",
    "        LOC = []\n",
    "        ORG = []\n",
    "        try:\n",
    "            text = Text(human)\n",
    "            for sent in text.sentences:\n",
    "                for e in sent.entities:\n",
    "                    if e.tag == 'I-LOC':\n",
    "                        LOC.append(e)\n",
    "                    elif e.tag == 'I-PER':\n",
    "                        PER.append(e)\n",
    "                    elif e.tag == 'I-ORG':\n",
    "                        ORG.append(e)            \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return PER, LOC, ORG\n",
    "    \n",
    "    def find_city(self, LOC):\n",
    "        # Capture if a city is given in the input\n",
    "         #Entities (cities, time, people)\n",
    "        \n",
    "        CITY = None\n",
    "        try: \n",
    "            if LOC[0][0] != None:\n",
    "                CITY = LOC[0][0]\n",
    "            if LOC[0][1] != None:\n",
    "                CITY = LOC[0][0] + \" \"+ LOC[0][1]\n",
    "        except:\n",
    "            pass\n",
    "        if CITY != None:\n",
    "            CITY = CITY.capitalize()\n",
    "\n",
    "        CITY2 = None\n",
    "        try: \n",
    "            if LOC[1][0] != None:\n",
    "                CITY2 = LOC[1][0]\n",
    "            if LOC[1][1] != None:\n",
    "                CITY2 = LOC[1][0] + \" \"+ LOC[1][1]\n",
    "        except:\n",
    "            pass\n",
    "        if CITY2 != None:\n",
    "            CITY2 = CITY2.capitalize()\n",
    "            \n",
    "        return CITY,CITY2\n",
    "\n",
    "    def find_people(self, PER):\n",
    "        PERSON = None\n",
    "        try: \n",
    "            if PER[0][0] != None:\n",
    "                PERSON = PER[0][0]\n",
    "            if PER[0][1] != None:\n",
    "                PERSON = PER[0][0] + \" \"+ PER[0][1]\n",
    "        except:\n",
    "            pass\n",
    "        if PERSON != None:\n",
    "            return PERSON\n",
    "    \n",
    "    def detector_language(self, detector, lang):\n",
    "        # we use Polyglot only if there is a conflict between IT and SP and we give precedence to Polyglot\n",
    "        if detector == 'Spanish' and lang == 'IT':\n",
    "            lang = 'ES'\n",
    "        elif detector == 'Italian' and lang == 'ES':\n",
    "            lang = 'IT'\n",
    "        return lang\n",
    "    \n",
    "\n",
    "    def check_bigram_city(self, CITY):\n",
    "        if CITY != None:\n",
    "            CITY = CITY.split()\n",
    "            try:\n",
    "                if CITY[1] != None:\n",
    "                    CITY = CITY[0].capitalize() + \" \" + CITY[1].capitalize()\n",
    "            except:\n",
    "                CITY = \"\".join(CITY).capitalize()\n",
    "                \n",
    "        return CITY\n",
    "        \n",
    "       \n",
    "    def talk(self):\n",
    "        \n",
    "        NER_dict = dict()\n",
    "        while True:\n",
    "            \n",
    "            human = input('YOU: ')\n",
    "            if human == 'bye':\n",
    "                break\n",
    "            \n",
    "            PER,LOC, ORG = self.NER(human)\n",
    "            leaving = None\n",
    "            arriving = None\n",
    "            detector = None\n",
    "            \n",
    "             # Language Detector\n",
    "            try:\n",
    "                detector = Detector(human).language.name\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            human = human.split()\n",
    "            hum = []\n",
    "\n",
    "            for tok in human:\n",
    "                hum.append(re.sub(r'[^\\w]', '', tok))\n",
    "            hum.append('EOS')\n",
    "\n",
    "            NER_dict['people'] = PER\n",
    "            NER_dict['cities'] = LOC\n",
    "            NER_dict['organizations'] = ORG\n",
    "            NER_dict['leaving'] = leaving\n",
    "            NER_dict['arriving'] = arriving\n",
    "            NER_dict['detector'] = detector\n",
    "            \n",
    "            return hum, NER_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLWEs = Vectors(['wiki.multi.en.vec.txt', 'wiki.multi.es.vec.txt', 'wiki.multi.de.vec.txt', 'wiki.multi.et.vec.txt',\n",
    "                     #'wiki.multi.it.vec.txt', 'wiki.multi.ru.vec.txt'])\n",
    "CLWEs = Vectors(['wiki.multi.en.vec.txt', 'wiki.multi.it.vec.txt'])\n",
    "all_vectors = CLWEs.paths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Conversation()\n",
    "m = MakeBot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_weather(NER_dict):\n",
    "    return print('HELLO!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords, dict_CLWEs, kw_lang, answer, bigrams=None, cutoff=0.43, boost=0.5, all_lang=None\n",
    "weatherbot = PolyBot(['sun', 'rain'], all_vectors, 'EN', answer_weather, bigrams=[('what', 'time'), ('what', 'is')])\n",
    "travelbot = PolyBot(['travel', 'car'], all_vectors, 'EN', answer_weather)\n",
    "bots = [weatherbot, travelbot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what', 'time'), ('what', 'is')]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weatherbot.bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv (bots):\n",
    "    \n",
    "    while True:\n",
    " #### CREATE DICTIONARIES ################################################################################################\n",
    "        bots_dict = dict()\n",
    "        for i, bot in enumerate(bots):\n",
    "            empty_dictionaries, confidence, multi, highest_token = bot.prepare_bot()\n",
    "            bots_dict['confidence'+str(i)] = confidence\n",
    "            bots_dict['empty_dictionaries'+str(i)] = empty_dictionaries\n",
    "            bots_dict['multi_dict'+str(i)] = multi\n",
    "            bots_dict['highest_token'+str(i)] = highest_token\n",
    "        \n",
    "    ### START CONVERSATION #######################################################################################################\n",
    "        try:\n",
    "            #NER\n",
    "            hum, NER_dict= c.talk()\n",
    "\n",
    "        except:\n",
    "            print('BOT: Have a nice day')\n",
    "            break\n",
    "\n",
    "        \n",
    "        for i in range (len(hum)-1):\n",
    "            # normalize the input\n",
    "            token = hum[i].lower()\n",
    "            next_token = hum[i+1].lower()\n",
    "             \n",
    "            # Assign the multilingual vectors to the input words\n",
    "            vect_dict = m.token2multi_vectors(token, all_vectors)\n",
    "            next_vect_dict= m.token2multi_vectors(next_token, all_vectors)\n",
    "            today,tomorrow= c.find_TodayTomorrow(vect_dict, all_vectors)\n",
    "            \n",
    "            \n",
    "            for i,bot in enumerate(bots):\n",
    "                empty_dictionaries = bots_dict['empty_dictionaries'+str(i)]\n",
    "                confidence =  bots_dict['confidence'+str(i)]\n",
    "                highest_token =  bots_dict['highest_token'+str(i)]\n",
    "                multi_dict = bots_dict['multi_dict'+str(i)]\n",
    "                language_bot = bot.language_identifier(empty_dictionaries, vect_dict, highest_token, multi_dict)\n",
    "                bot.get_confidence(vect_dict, confidence, multi_dict, language_bot)\n",
    "                bots_dict['language_bot'+str(i)] = language_bot\n",
    "                \n",
    "                if bot.bigrams != None:\n",
    "                    for i in range(len(bot.bigrams)):\n",
    "                        #  w1, w2, dict_multivector, dict_next_multivector, confidence\n",
    "                        w1 = bot.bigrams[i][0]\n",
    "                        w2 = bot.bigrams[i][1]\n",
    "                        is_true = bot.compute_bigrams(w1,w2, vect_dict, next_vect_dict, confidence)\n",
    "                        bots_dict[w1 + '_' + w2] = is_true\n",
    "            \n",
    "        all_confs = dict()\n",
    "        for i, bot in enumerate(bots):\n",
    "            confidence =  bots_dict['confidence'+str(i)]\n",
    "            summed_conf = sum(confidence)\n",
    "            all_confs['conf'+str(i)] = summed_conf\n",
    "\n",
    "        sorted_confs = sorted(all_confs.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "        print(sorted_confs, 'sorted CONFS')    \n",
    "            \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOU: sun\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detector is not able to detect the language reliably.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('conf0', 1.5), ('conf1', 0)] sorted CONFS\n",
      "YOU: sun sun sun travel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detector is not able to detect the language reliably.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('conf0', 4.5), ('conf1', 1.5000000000000002)] sorted CONFS\n",
      "YOU: bye\n",
      "BOT: Have a nice day\n"
     ]
    }
   ],
   "source": [
    "conv(bots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

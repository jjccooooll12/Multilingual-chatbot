{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import operator\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import polyglot\n",
    "from polyglot.downloader import downloader\n",
    "from polyglot.text import Text\n",
    "import wikipedia\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "from polyglot.detect import Detector\n",
    "from datetime import datetime, date\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the cross lingual word embeddings (CLWEs)\n",
    "en_path = 'wiki.multi.en.vec.txt'\n",
    "sp_path = 'wiki.multi.es.vec.txt'\n",
    "it_path = 'wiki.multi.it.vec.txt'\n",
    "est_path = 'wiki.multi.et.vec.txt'\n",
    "rus_path = 'wiki.multi.ru.vec.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that transforms the parallel data into vectors\n",
    "def make_vectors(directory):\n",
    "    table = dict()\n",
    "    with open(directory, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        next(f)\n",
    "        vectors = []\n",
    "        for i, line in enumerate(f):\n",
    "            word,vect = line.rstrip().split(\" \",1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            table[word] = vect\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the CLWEs for english, spanish, italian, estonian, russian\n",
    "table_eng = make_vectors(en_path)\n",
    "table_sp = make_vectors(sp_path)\n",
    "table_it = make_vectors(it_path)\n",
    "table_est = make_vectors(est_path)\n",
    "table_rus = make_vectors(rus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that transforms tokens into vectors\n",
    "def token2multi_vectors(token):\n",
    "        # Assign the multilingual vectors to the input words\n",
    "        # assign a zero confidence to stopwords\n",
    "        try:\n",
    "            vect = table_eng[token].reshape(1,300)   \n",
    "        except:\n",
    "            vect = UNK\n",
    "        try:\n",
    "            vect_sp = table_sp[token].reshape(1,300)\n",
    "        except:\n",
    "            vect_sp =UNK\n",
    "        try:\n",
    "            vect_it = table_it[token].reshape(1,300)\n",
    "        except:\n",
    "            vect_it = UNK\n",
    "        try:\n",
    "            vect_rus = table_rus[token].reshape(1,300)\n",
    "        except:\n",
    "            vect_rus = UNK\n",
    "        try:\n",
    "            vect_est = table_est[token].reshape(1,300)\n",
    "        except:\n",
    "            vect_est = UNK\n",
    "        return vect, vect_sp, vect_it, vect_rus, vect_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that intercepts the vectors related to the time of the day\n",
    "def when_def(when_dict, conf_value, languages):\n",
    "    sim_today = []\n",
    "    for key, value in when_dict.items():\n",
    "        value = value.reshape(1,300)\n",
    "        for lang in languages:\n",
    "            sim = cosine_similarity(lang, value)\n",
    "            if sim > conf_value:\n",
    "                sim_today.append(sim)\n",
    "    return sum(sim_today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_city(from_phrase, parola, lang):\n",
    "    prep = 0\n",
    "    print(from_phrase) \n",
    "    city = input('YOU: ')\n",
    "    tok = city.lower().split()\n",
    "    for i in range(len(tok)):\n",
    "        vect, vect_sp, vect_it,vect_rus,vect_est = token2multi_vectors(tok[i])\n",
    "        if lang == 'en':\n",
    "            if cosine_similarity(vect, table_eng[parola].reshape(1,300)) > 0.8:\n",
    "                prep += 1\n",
    "        elif lang == 'es':\n",
    "            if cosine_similarity(vect_sp, table_eng[parola].reshape(1,300)) > 0.3:\n",
    "                prep += 1\n",
    "        elif lang == 'it':\n",
    "            if cosine_similarity(vect_it, table_eng[parola].reshape(1,300)) > 0.3:\n",
    "                prep += 1\n",
    "        elif lang == 'et':\n",
    "            if cosine_similarity(vect_est, table_eng[parola].reshape(1,300)) > 0.3:\n",
    "                prep += 1\n",
    "        elif lang == 'ru':\n",
    "            if cosine_similarity(vect_rus, table_eng[parola].reshape(1,300)) > 0.3:\n",
    "                prep += 1\n",
    "    if prep > 0: \n",
    "        try:\n",
    "            if tok[2] != None:\n",
    "                CITY = tok[1].capitalize() + \" \" + tok[2].capitalize()      \n",
    "        except:\n",
    "            CITY = tok[1].capitalize()    \n",
    "    else:\n",
    "        CITY = city.capitalize()\n",
    "    return CITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WeatherBot answer\n",
    "\n",
    "def answer_weather(today, tomorrow, phrase_locality, phrase_when,  TODAY, TOMORROW, CITY, sorry_answer, lang):\n",
    "    BOT = 'BOT:'\n",
    "    if CITTA == False:\n",
    "        CITY = prep_city(phrase_locality, 'in', lang)          \n",
    "    else:\n",
    "        try:\n",
    "            CITY = CITY.split()\n",
    "            if CITY[1] != None:\n",
    "                CITY = CITY[0].capitalize() + \" \" + CITY[1].capitalize()\n",
    "        except:\n",
    "            CITY = \"\".join(CITY).capitalize()\n",
    "    \n",
    "    if today == False and tomorrow == False:\n",
    "        ty = 0\n",
    "        tw = 0\n",
    "        print(BOT, phrase_when)\n",
    "        when = input('YOU: ').lower().split()\n",
    "        for i in range(len(when)):\n",
    "            vect, vect_sp, vect_it,vect_rus,vect_est = token2multi_vectors(when[i])\n",
    "            languages = [vect, vect_est, vect_it, vect_rus, vect_sp]\n",
    "            oggi = when_def(today_key, 0.5, languages)\n",
    "            domani = when_def(tomorrow_key, 0.5, languages)\n",
    "            if oggi > 0 and oggi > domani:\n",
    "                ty +=1\n",
    "            if domani > 0 and domani > oggi:\n",
    "                tw +=1\n",
    "\n",
    "        if ty > 0:\n",
    "            today = TODAY\n",
    "            try:\n",
    "                url = \"https://openweathermap.org/data/2.5/weather?q={}&appid=b6907d289e10d714a6e88b30761fae22\".format(CITY)\n",
    "                res = requests.get(url)\n",
    "                data = res.json()\n",
    "                temp = data['main']['temp']\n",
    "                sky = data['weather'][0]['description']\n",
    "                if lang != 'en':\n",
    "                    print(BOT, translator.translate(\"There is {} and there are {} degrees in {} {}\".format(sky,\n",
    "                    temp,  CITY, today), dest=lang).text)\n",
    "                else:\n",
    "                    print(BOT, \"There is {} and there are {} degrees in {} {}\".format(sky, temp,CITY, today))\n",
    "            except:\n",
    "                print(BOT,sorry_answer)\n",
    "\n",
    "        elif tw > 0:\n",
    "            tomorrow = TOMORROW\n",
    "            try:\n",
    "                url = \"https://openweathermap.org/data/2.5/weather?q={}&appid=b6907d289e10d714a6e88b30761fae22\".format(CITY)\n",
    "                res = requests.get(url)\n",
    "                data = res.json()\n",
    "                temp = data['main']['temp']\n",
    "                sky = data['weather'][0]['description']\n",
    "                if lang != 'en':\n",
    "                    print(BOT, translator.translate(\"There is {} and there are {} degrees in {} {}\".format(sky,\n",
    "                    temp,  city, tomorrow), dest=lang).text)\n",
    "                else:\n",
    "                    print(BOT, \"There is {} and there are {} degrees in {} {}\".format(sky, temp,  CITY, tomorrow))\n",
    "            except:\n",
    "                print(BOT, sorry_answer)\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                url = \"https://openweathermap.org/data/2.5/weather?q={}&appid=b6907d289e10d714a6e88b30761fae22\".format(CITY)\n",
    "                res = requests.get(url)\n",
    "                data = res.json()\n",
    "                temp = data['main']['temp']\n",
    "                sky = data['weather'][0]['description']\n",
    "                if lang != 'en':\n",
    "                    print(BOT, translator.translate(\"In this day there is {} and there are {} degrees in {}\".format(sky, temp, \n",
    "                    CITY), dest=lang).text)\n",
    "                else:\n",
    "                    print(BOT, \"In this day there is {} and there are {} degrees in {}\".format(sky, temp,  CITY))\n",
    "            except:\n",
    "                print(BOT, sorry_answer)\n",
    "\n",
    "    elif today == True and tomorrow == False:\n",
    "        today = TODAY\n",
    "        try:\n",
    "            url = \"https://openweathermap.org/data/2.5/weather?q={}&appid=b6907d289e10d714a6e88b30761fae22\".format(CITY)\n",
    "            res = requests.get(url)\n",
    "            data = res.json()\n",
    "            temp = data['main']['temp']\n",
    "            sky = data['weather'][0]['description']\n",
    "            if lang != 'en':\n",
    "                    print(BOT, translator.translate(\"There is {} and there are {} degrees in {} {}\".format(sky,\n",
    "                    temp,  CITY, today), dest=lang).text)\n",
    "            else:\n",
    "                print(BOT, \"There is {} and there are {} degrees in {} {}\".format(sky, temp,  CITY, today))\n",
    "        except:\n",
    "            print(BOT, sorry_answer)\n",
    "\n",
    "    elif tomorrow == True:\n",
    "        tomorrow = TOMORROW\n",
    "        \n",
    "        try:\n",
    "            url = \"https://openweathermap.org/data/2.5/weather?q={}&appid=b6907d289e10d714a6e88b30761fae22\".format(CITY)\n",
    "            res = requests.get(url)\n",
    "            data = res.json()\n",
    "            temp = data['main']['temp']\n",
    "            sky = data['weather'][0]['description']\n",
    "            if lang != 'en':\n",
    "                    print(BOT, translator.translate(\"There is {} and there are {} degrees in {} {}\".format(sky,\n",
    "                    temp,  CITY, tomorrow), dest=lang).text)\n",
    "            else:\n",
    "                print(BOT, \"There is {} and there are {} degrees in {} {}\".format(sky, temp,  CITY, tomorrow))\n",
    "        except:\n",
    "            print(BOT, sorry_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wiki answers\n",
    "\n",
    "def answer_wiki(frase_sorry, frase_noinfo):\n",
    "    \n",
    "    if WHAT == True:\n",
    "        try:\n",
    "            res = \".\".join(wikipedia.summary(wikipedia.search(topic, results=1)).split('.')[0:2])\n",
    "            print('BOT:', res)\n",
    "        except:\n",
    "            print(frase_sorry)\n",
    "            response = input('YOU: ').lower()\n",
    "            try:\n",
    "                res = \".\".join(wikipedia.summary(wikipedia.search(response, results=1)).split('.')[0:2])\n",
    "                print(\"BOT:\", res)\n",
    "            except:\n",
    "                print(frase_noinfo, response)\n",
    "    else:   \n",
    "        try:\n",
    "            if person == True:\n",
    "                res = \".\".join(wikipedia.summary(wikipedia.search(PERSON, results=1)).split('.')[0:2])\n",
    "                print('BOT:', res)\n",
    "            elif CITTA == True:\n",
    "                res = \".\".join(wikipedia.summary(wikipedia.search(CITY, results=1)).split('.')[0:2])\n",
    "                print('BOT:', res)\n",
    "            else:\n",
    "                print(frase_sorry)\n",
    "                response = input('YOU: ').lower()\n",
    "                try:\n",
    "                    res = \".\".join(wikipedia.summary(wikipedia.search(response, results=1)).split('.')[0:2])\n",
    "                    print(\"BOT:\", res)\n",
    "                except:\n",
    "                    print(frase_noinfo, response)\n",
    "        except:\n",
    "            print(frase_sorry)\n",
    "            response = input('YOU: ').lower()\n",
    "            try:\n",
    "                res = \".\".join(wikipedia.summary(wikipedia.search(response, results=1)).split('.')[0:2])\n",
    "                print(\"BOT:\", res)\n",
    "            except:\n",
    "                print(frase_noinfo, response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Travelbot answers\n",
    "def answer_travel(CITY, CITY2, leaving, arriving, today, tomorrow, lang, destination_phrase, from_city_phrase, sorry_phrase, when_phrase, oggi, domani):\n",
    "\n",
    "    if CITY != None:\n",
    "        CITY = CITY.split()\n",
    "        try:\n",
    "            if CITY[1] != None:\n",
    "                CITY = CITY[0].capitalize() + \" \" + CITY[1].capitalize()\n",
    "        except:\n",
    "            CITY = \"\".join(CITY).capitalize()\n",
    "    if CITY2 != None:\n",
    "        CITY2 = CITY2.split()\n",
    "        try:\n",
    "            if CITY2[1] != None:\n",
    "                CITY2 = CITY2[0].capitalize() + \" \" + CITY2[1].capitalize()\n",
    "        except:\n",
    "            CITY2 = \"\".join(CITY2).capitalize()\n",
    "\n",
    "    if CITY2 !=None:\n",
    "        if leaving == arriving:\n",
    "            print(destination_phrase)\n",
    "            tok = input('YOU: ').lower().capitalize().split()\n",
    "            try:\n",
    "                if tok[1] != None:\n",
    "                    tok = tok[0].capitalize() + \" \" + tok[1].capitalize()  \n",
    "            except:\n",
    "                tok = tok[0].capitalize()  \n",
    "            if tok[:3] == CITY[:3]:\n",
    "                arriving = CITY\n",
    "                leaving = CITY2\n",
    "            else:\n",
    "                arriving = CITY2\n",
    "                leaving = CITY\n",
    "        \n",
    "        if leaving != None:\n",
    "            leaving = leaving\n",
    "        else:\n",
    "            where2start = prep_city(from_city_phrase, 'from', lang)\n",
    "            # we just confront the first 3 letters of the 2 cities (in case of languages wt different suffixies)\n",
    "            if where2start[:3] == CITY2[:3]:\n",
    "                arriving = CITY\n",
    "                leaving = CITY2\n",
    "            elif where2start[:3] == CITY[:3]:\n",
    "                arriving = CITY2\n",
    "                leaving = CITY\n",
    "            else:\n",
    "                print(sorry_phrase)\n",
    "                today = True\n",
    "\n",
    "        if leaving != None and arriving == None:\n",
    "            if leaving == CITY:\n",
    "                arriving = CITY2\n",
    "            elif leaving ==CITY2:\n",
    "                arriving = CITY\n",
    "            else:\n",
    "                print(destination_phrase)\n",
    "                arriving = input('YOU: ').lower().capitalize().split()\n",
    "                try:\n",
    "                    if arriving[1] != None:\n",
    "                        arriving = arriving[0].capitalize() + \" \" + arriving[1].capitalize()  \n",
    "                except:\n",
    "                    arriving = arriving[0].capitalize() \n",
    "\n",
    "        if arriving != None and leaving == None:\n",
    "            if arriving == CITY:\n",
    "                leaving = CITY2\n",
    "            elif arriving == CITY:\n",
    "                leaving = CITY\n",
    "            else:\n",
    "                leaving = prep_city(from_city_phrase, 'from', lang)\n",
    "\n",
    "    if CITY != None and CITY2 == None:\n",
    "        if arriving != None:\n",
    "            arriving = CITY\n",
    "            leaving = prep_city(from_city_phrase, 'from', lang)\n",
    "        elif leaving != None:\n",
    "            leaving = CITY\n",
    "            print(destination_phrase)\n",
    "            arriving = input('YOU: ').lower().capitalize().split()\n",
    "            try:\n",
    "                if arriving[1] != None:\n",
    "                    arriving = arriving[0].capitalize() + \" \" + arriving[1].capitalize()  \n",
    "            except:\n",
    "                arriving = arriving[0].capitalize()\n",
    "        else:\n",
    "            where2start = prep_city(from_city_phrase, 'from', lang)\n",
    "            # we just confront the first 3 letters of the 2 cities (in case of languages wt different suffixies)\n",
    "            if where2start[:3] == CITY[:3]:\n",
    "                leaving = CITY\n",
    "                arriving = input('YOU: ').lower().capitalize().split()\n",
    "                try:\n",
    "                    if arriving[1] != None:\n",
    "                        arriving = arriving[0].capitalize() + \" \" + arriving[1].capitalize()  \n",
    "                except:\n",
    "                    arriving = arriving[0].capitalize()\n",
    "                    \n",
    "            else:\n",
    "                arriving = CITY\n",
    "                leaving = where2start\n",
    "            \n",
    "\n",
    "    elif CITY == None:\n",
    "        leaving = prep_city(from_city_phrase, 'from', lang)\n",
    "        \n",
    "        arriving = input('YOU: ').lower().capitalize().split()\n",
    "        try:\n",
    "            if arriving[1] != None:\n",
    "                arriving = arriving[0].capitalize() + \" \" + arriving[1].capitalize()  \n",
    "        except:\n",
    "            arriving = arriving[0].capitalize()\n",
    "\n",
    "    if today == False and tomorrow == False:\n",
    "        ty = 0\n",
    "        tw = 0\n",
    "        print(when_phrase)\n",
    "        day = input('YOU: ').lower()\n",
    "        for i in range(len(day)):\n",
    "            vect, vect_sp, vect_it,vect_rus,vect_est = token2multi_vectors(day[i])\n",
    "            languages = [vect, vect_est, vect_it, vect_rus, vect_sp]\n",
    "            oggi = when_def(today_key, 0.5, languages)\n",
    "            domani = when_def(tomorrow_key, 0.5, languages)\n",
    "            if oggi > 0 and oggi > domani:\n",
    "                ty +=1\n",
    "            if domani > 0 and domani > oggi:\n",
    "                tw +=1\n",
    "        if ty > 0:\n",
    "            today = oggi\n",
    "        elif tw > 0:\n",
    "            tomorrow = domani\n",
    "    else:\n",
    "        if today == True:\n",
    "            day = oggi\n",
    "        elif tomorrow == True:\n",
    "            day = domani\n",
    "    \n",
    "    if lang != 'en':\n",
    "        if lang == 'ru':\n",
    "            print(\"Я нахожу лучший путь от {} до {} {}\".format(leaving, arriving, day))\n",
    "        else:\n",
    "            print(\"BOT:\", translator.translate(\"I am Finding the best way to go from {} to {} {}\".format(leaving, arriving, day), dest=lang).text)\n",
    "        \n",
    "    else:\n",
    "        print(\"BOT: Finding the best way to go from {} to {} {}\".format(leaving, arriving, day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to wikify the entity\n",
    "def selenium (url):\n",
    "    chrome_options = Options() \n",
    "    chrome_options.add_argument(\"--headless\") \n",
    "    driver = webdriver.Chrome(r'C:/Users/Jason/Downloads/chromedriver_win32/chromedriver.exe', options=chrome_options)\n",
    "    driver.get(url)\n",
    "    \n",
    "    driver.find_element_by_link_text('English').click()\n",
    "    name= driver.title\n",
    "    LANGS = [\n",
    "    'ÁáČčĎďÉéĚěÍíŇňÓóŘřŠšŤťÚúŮůÝýŽž',   # Czech\n",
    "    'ÄäÖöÜüẞß',                         # German\n",
    "    'ĄąĆćĘęŁłŃńÓóŚśŹźŻż',               # Polish\n",
    "    ]\n",
    "    pattern = r'[A-Z][a-z{langs}]+'.format(langs=''.join(LANGS))\n",
    "    pattern = re.compile(pattern)\n",
    "    match = pattern.findall(name)\n",
    "    match = match[0:-1]\n",
    "    try:\n",
    "        if match [1] != None:\n",
    "            return match[0] + \" \" + match[1]\n",
    "    except:\n",
    "        return match[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that captures the highest similarity within the token and the keywords for a language\n",
    "def highest_similarity_for_keyword(similarity_language, vect):\n",
    "    last_status = similarity_language[key]\n",
    "    similarity = cosine_similarity(vect, value)\n",
    "    if similarity > last_status:\n",
    "        similarity_language[key] = similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that compares the similarities between all languages and returns the highest one overall\n",
    "def highest_sim_over_language(simbot_en, simbot_sp, simbot_it, simbot_ru, simbot_est, multibot):\n",
    "    #Eng\n",
    "    highest_similarity_for_keyword(simbot_en, vect)\n",
    "    #Sp\n",
    "    highest_similarity_for_keyword(simbot_sp, vect_sp)\n",
    "    #It\n",
    "    highest_similarity_for_keyword(simbot_it, vect_it)\n",
    "    #Rus\n",
    "    highest_similarity_for_keyword(simbot_ru, vect_rus)\n",
    "    #Est\n",
    "    highest_similarity_for_keyword(simbot_est, vect_est)\n",
    "\n",
    "    sorted_simil_weather_en = sorted(simbot_en.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_simil_weather_sp = sorted(simbot_sp.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_simil_weather_it = sorted(simbot_it.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_simil_weather_ru = sorted(simbot_ru.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_simil_weather_est = sorted(simbot_est.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    multibot[sorted_simil_weather_en[0][0], 'EN'] = sorted_simil_weather_en[0][1]\n",
    "    multibot[sorted_simil_weather_sp[0][0], 'SP' ] = sorted_simil_weather_sp[0][1]\n",
    "    multibot[sorted_simil_weather_it[0][0], 'IT'] = sorted_simil_weather_it[0][1]\n",
    "    multibot[sorted_simil_weather_ru[0][0], 'RU'] = sorted_simil_weather_ru[0][1]\n",
    "    multibot[sorted_simil_weather_est[0][0], 'EST'] = sorted_simil_weather_est[0][1]\n",
    "\n",
    "    # sort the keys by the highest cosine values of all languages\n",
    "    sorted_simil_weather = sorted(multibot.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    return sorted_simil_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that looks for bigram with high cosine similarity for the WikiBot\n",
    "def wiki_bigram(wh, conf):\n",
    "    languages = [vect_est, vect_it, vect_rus, vect_sp]\n",
    "    languages_next = [next_vect_est, next_vect_it, next_vect_rus, next_vect_sp]\n",
    "    for i in range(len(languages)):\n",
    "        if cosine_similarity(table_eng[str(wh)].reshape(1,300), languages[i]) + cosine_similarity(table_eng['is'].reshape(1,300), languages_next[i]) > 0.8:\n",
    "            confidence_wiki.append(conf)\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that looks for bigram with high cosine similarity for the WikiBot\n",
    "def wiki_weather(wh, conf):\n",
    "    languages = [vect_est, vect_it, vect_rus, vect_sp]\n",
    "    languages_next = [next_vect_est, next_vect_it, next_vect_rus, next_vect_sp]\n",
    "    for i in range(len(languages)):\n",
    "        if cosine_similarity(table_eng[str(wh)].reshape(1,300), languages[i]) + cosine_similarity(table_eng['has'].reshape(1,300), languages_next[i]) > 0.8:\n",
    "            confidence_wiki.append(conf)\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6386973]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(table_eng['time'].reshape(1,300), table_it['tempo'].reshape(1,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.48072084]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(table_eng['to'].reshape(1,300), table_it['a'].reshape(1,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrmas would not work in Russian:\n",
    "#который сейчас час?\n",
    "# Just a keyword will not work in Estonian:\n",
    "# Mis kell on\n",
    "# -> so let's have a really high cutoff for keywords so they are note confused by when keywords for language like russian\n",
    "# and bigrams for language like Estonian\n",
    "\n",
    "\n",
    "# A function that looks for bigram with high cosine similarity for the TimeBot\n",
    "def time_bigram():\n",
    "    languages = [vect_est, vect_it, vect_rus, vect_sp]\n",
    "    languages_next = [next_vect_est, next_vect_it, next_vect_rus, next_vect_sp] \n",
    "    for i in range(len(languages)):\n",
    "        sim1 = cosine_similarity(table_eng['what'].reshape(1,300), languages[i])\n",
    "        sim2 = cosine_similarity(table_eng['hours'].reshape(1,300), languages_next[i])\n",
    "        print(sim1 + sim2, 'RESULTS')\n",
    "        if  sim1 +  sim2 > 0.75:\n",
    "            confidence_time.append(5)\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that looks for bigram with high cosine similarity for the WeatherBot\n",
    "def travel_bigram (FROM, which_city, conf):\n",
    "    if which_city != None:\n",
    "        languages = [vect_est, vect_it, vect_rus, vect_sp]\n",
    "        for i in range(len(languages)):\n",
    "            if cosine_similarity(table_eng[str(FROM)].reshape(1,300), languages[i]) > conf and next_token.capitalize() == which_city:\n",
    "                leaving = which_city\n",
    "                return leaving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that identifies the input language\n",
    "def language_identifier(sorted_simil):\n",
    "    if sorted_simil[0][0][1] == 'EN':\n",
    "        language = 'EN'\n",
    "    elif sorted_simil[0][0][1] == 'SP':\n",
    "        language = 'SP'\n",
    "    elif sorted_simil[0][0][1] == 'IT':\n",
    "        language = 'IT'\n",
    "    elif sorted_simil[0][0][1] == 'RU':\n",
    "        language = 'RU'\n",
    "    elif sorted_simil[0][0][1] == 'EST':\n",
    "        language = 'EST'\n",
    "    return language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE CHATBOT\n",
    "\n",
    "##### KEYWORDS #############################################################################################################\n",
    "# keywords for weather_bot\n",
    "keywords_weather = {'snow':table_eng['snow'], 'rain':table_eng['rain'], 'weather':table_eng['weather'], \n",
    "                    'sun':table_eng['sun'], 'hailstorm':table_eng['hailstorm'], 'fog':table_eng['fog'], \n",
    "                   'temperature':table_eng['temperature'], 'degree':table_eng['degree'], 'cold': table_eng['cold'],\n",
    "                   'hot':table_eng['hot'], 'warm':table_eng['warm']}\n",
    "\n",
    "# keywords for Wikipedia\n",
    "keywords_wikipedia = {'wikipedia':table_eng['wikipedia'], 'meaning': table_eng['meaning'], 'mean': table_eng['mean'], \n",
    "                     'sense': table_eng['sense']}\n",
    "\n",
    "# keywords for travel_bot\n",
    "keywords_travel = {'travel': table_eng['travel'], 'trip':table_eng['trip'], 'ride':table_eng['ride'], 'fly':table_eng['fly'],\n",
    "                  'go':table_eng['go'], 'drive':table_eng['drive'], 'bus':table_eng['bus'], 'plane': table_eng['plane'],\n",
    "                  'taxi':table_eng['taxi'], 'ferry': table_eng['ferry'], 'car':table_eng['car'], 'train': table_eng['train']}\n",
    "\n",
    "# keywords for time_bot\n",
    "keywords_time = {'time': table_eng['time'], 'hours': table_eng['hours'] }\n",
    "\n",
    "today_key = {'today':table_eng['today'],  'now':table_eng['now']}\n",
    "tomorrow_key = {'tomorrow':table_eng['tomorrow']}\n",
    "\n",
    "\n",
    "#############################################################################################################################\n",
    "\n",
    "# Vectors not found in MUSE\n",
    "UNK = np.zeros((1, 300))\n",
    "\n",
    "while True:\n",
    "    \n",
    "    #Entities (cities, time, people)\n",
    "    CITTA = False\n",
    "    CITTA2 = False\n",
    "    today = False\n",
    "    tomorrow = False\n",
    "    person = False\n",
    "    yesterday = False\n",
    "    WHAT = False\n",
    "    partenza = None\n",
    "    arrivo = None\n",
    "    leaving = None\n",
    "    arriving = None\n",
    "   \n",
    "    \n",
    "    \n",
    "##### EMPTY DICTIONARIES ####################################################################################################\n",
    "\n",
    "    # Empty dictionaries for the WEATHER BOT\n",
    "    confidence_weather = []\n",
    "    tokens_weather_simil = dict()\n",
    "    multi_weather = dict()\n",
    "\n",
    "    similarities_weather_en = {'snow':np.asarray([[0]]), 'rain':np.asarray([[0]]), 'weather':np.asarray([[0]]), \n",
    "                    'sun':np.asarray([[0]]), 'hailstorm':np.asarray([[0]]), 'fog':np.asarray([[0]]), \n",
    "                   'temperature':np.asarray([[0]]), 'degree':np.asarray([[0]]), 'cold':np.asarray([[0]]),\n",
    "                   'hot':np.asarray([[0]]), 'warm':np.asarray([[0]])}\n",
    "    similarities_weather_sp = {'snow':np.asarray([[0]]), 'rain':np.asarray([[0]]), 'weather':np.asarray([[0]]), \n",
    "                    'sun':np.asarray([[0]]), 'hailstorm':np.asarray([[0]]), 'fog':np.asarray([[0]]), \n",
    "                   'temperature':np.asarray([[0]]), 'degree':np.asarray([[0]]), 'cold':np.asarray([[0]]),\n",
    "                   'hot':np.asarray([[0]]), 'warm':np.asarray([[0]])}\n",
    "    similarities_weather_it = {'snow':np.asarray([[0]]), 'rain':np.asarray([[0]]), 'weather':np.asarray([[0]]), \n",
    "                    'sun':np.asarray([[0]]), 'hailstorm':np.asarray([[0]]), 'fog':np.asarray([[0]]), \n",
    "                   'temperature':np.asarray([[0]]), 'degree':np.asarray([[0]]), 'cold':np.asarray([[0]]),\n",
    "                   'hot':np.asarray([[0]]), 'warm':np.asarray([[0]])}\n",
    "    similarities_weather_ru = {'snow':np.asarray([[0]]), 'rain':np.asarray([[0]]), 'weather':np.asarray([[0]]), \n",
    "                    'sun':np.asarray([[0]]), 'hailstorm':np.asarray([[0]]), 'fog':np.asarray([[0]]), \n",
    "                   'temperature':np.asarray([[0]]), 'degree':np.asarray([[0]]), 'cold':np.asarray([[0]]),\n",
    "                   'hot':np.asarray([[0]]), 'warm':np.asarray([[0]])}\n",
    "    similarities_weather_est = {'snow':np.asarray([[0]]), 'rain':np.asarray([[0]]), 'weather':np.asarray([[0]]), \n",
    "                    'sun':np.asarray([[0]]), 'hailstorm':np.asarray([[0]]), 'fog':np.asarray([[0]]), \n",
    "                   'temperature':np.asarray([[0]]), 'degree':np.asarray([[0]]), 'cold':np.asarray([[0]]),\n",
    "                   'hot':np.asarray([[0]]), 'warm':np.asarray([[0]])}\n",
    "    \n",
    "    # Empty dictionaries for WIKIPEDIA BOT\n",
    "    confidence_wiki = []\n",
    "    tokens_wiki_simil = dict()\n",
    "    multi_wiki = dict()\n",
    "    similarities_wiki_en = {'wikipedia':np.asarray([[0]]), 'meaning':np.asarray([[0]]), 'mean':np.asarray([[0]]), \n",
    "                    'sense':np.asarray([[0]])}#, 'who': np.asarray([[0]]), 'is': np.asarray([[0]])}\n",
    "    similarities_wiki_sp = {'wikipedia':np.asarray([[0]]), 'meaning':np.asarray([[0]]), 'mean':np.asarray([[0]]), \n",
    "                    'sense':np.asarray([[0]])}#, 'who': np.asarray([[0]]), 'is': np.asarray([[0]])}\n",
    "    similarities_wiki_it = {'wikipedia':np.asarray([[0]]), 'meaning':np.asarray([[0]]), 'mean':np.asarray([[0]]), \n",
    "                    'sense':np.asarray([[0]])}#, 'who': np.asarray([[0]]), 'is': np.asarray([[0]])}\n",
    "    similarities_wiki_ru = {'wikipedia':np.asarray([[0]]), 'meaning':np.asarray([[0]]), 'mean':np.asarray([[0]]), \n",
    "                    'sense':np.asarray([[0]])}#, 'who': np.asarray([[0]]), 'is': np.asarray([[0]])}\n",
    "    similarities_wiki_est = {'wikipedia':np.asarray([[0]]), 'meaning':np.asarray([[0]]), 'mean':np.asarray([[0]]), \n",
    "                    'sense':np.asarray([[0]])}#, 'who': np.asarray([[0]]), 'is': np.asarray([[0]])}\n",
    "    \n",
    "    # Empty dictionaries for TRAVEL BOT\n",
    "    confidence_travel = []\n",
    "    tokens_travel_simil = dict()\n",
    "    multi_travel = dict()\n",
    "    similarities_travel_en = {'travel': np.asarray([[0]]), 'trip':np.asarray([[0]]), 'ride':np.asarray([[0]]), \n",
    "                'fly':np.asarray([[0]]),'go':np.asarray([[0]]), 'drive':np.asarray([[0]]), 'bus':np.asarray([[0]]),\n",
    "                'plane': np.asarray([[0]]),'taxi':np.asarray([[0]]), 'ferry': np.asarray([[0]]), 'car':np.asarray([[0]]), \n",
    "                'train': np.asarray([[0]])}\n",
    "    similarities_travel_sp = {'travel': np.asarray([[0]]), 'trip':np.asarray([[0]]), 'ride':np.asarray([[0]]), \n",
    "                'fly':np.asarray([[0]]),'go':np.asarray([[0]]), 'drive':np.asarray([[0]]), 'bus':np.asarray([[0]]),\n",
    "                'plane': np.asarray([[0]]),'taxi':np.asarray([[0]]), 'ferry': np.asarray([[0]]), 'car':np.asarray([[0]]), \n",
    "                'train': np.asarray([[0]])}\n",
    "    similarities_travel_it = {'travel': np.asarray([[0]]), 'trip':np.asarray([[0]]), 'ride':np.asarray([[0]]), \n",
    "                'fly':np.asarray([[0]]),'go':np.asarray([[0]]), 'drive':np.asarray([[0]]), 'bus':np.asarray([[0]]),\n",
    "                'plane': np.asarray([[0]]),'taxi':np.asarray([[0]]), 'ferry': np.asarray([[0]]), 'car':np.asarray([[0]]), \n",
    "                'train': np.asarray([[0]])}\n",
    "    similarities_travel_ru = {'travel': np.asarray([[0]]), 'trip':np.asarray([[0]]), 'ride':np.asarray([[0]]), \n",
    "                'fly':np.asarray([[0]]),'go':np.asarray([[0]]), 'drive':np.asarray([[0]]), 'bus':np.asarray([[0]]),\n",
    "                'plane': np.asarray([[0]]),'taxi':np.asarray([[0]]), 'ferry': np.asarray([[0]]), 'car':np.asarray([[0]]), \n",
    "                'train': np.asarray([[0]])}\n",
    "    similarities_travel_est = {'travel': np.asarray([[0]]), 'trip':np.asarray([[0]]), 'ride':np.asarray([[0]]), \n",
    "                'fly':np.asarray([[0]]),'go':np.asarray([[0]]), 'drive':np.asarray([[0]]), 'bus':np.asarray([[0]]),\n",
    "                'plane': np.asarray([[0]]),'taxi':np.asarray([[0]]), 'ferry': np.asarray([[0]]), 'car':np.asarray([[0]]), \n",
    "                'train': np.asarray([[0]])}\n",
    "    \n",
    "    # Empty dictionaries for TIME BOT\n",
    "    confidence_time = []\n",
    "    multi_time = dict()\n",
    "    similarities_time_en = {'hours':np.asarray([[0]]), 'time':np.asarray([[0]])}\n",
    "    similarities_time_sp = {'hours':np.asarray([[0]]), 'time':np.asarray([[0]])}\n",
    "    similarities_time_it = {'hours':np.asarray([[0]]), 'time':np.asarray([[0]])}\n",
    "    similarities_time_ru = {'hours':np.asarray([[0]]), 'time':np.asarray([[0]])}\n",
    "    similarities_time_est = {'hours':np.asarray([[0]]), 'time':np.asarray([[0]])}\n",
    "     \n",
    "######## START THE CONVERSATION #############################################################################################\n",
    "    \n",
    "    human = input('YOU: ')\n",
    "    \n",
    "    # break\n",
    "    if human == 'bye':\n",
    "        break\n",
    "    \n",
    "######### NER ##############################################################################################################\n",
    "    LOC = []\n",
    "    PER = []\n",
    "    try:\n",
    "        text = Text(human)\n",
    "        for sent in text.sentences:\n",
    "            for e in sent.entities:\n",
    "                if e.tag == 'I-LOC':\n",
    "                    LOC.append(e)\n",
    "                elif e.tag == 'I-PER':\n",
    "                    PER.append(e)\n",
    "    except:\n",
    "        pass\n",
    "    # Capture if a city is given in the input\n",
    "    CITY = None\n",
    "    try: \n",
    "        if LOC[0][0] != None:\n",
    "            CITY = LOC[0][0]\n",
    "        if LOC[0][1] != None:\n",
    "            CITY = LOC[0][0] + \" \"+ LOC[0][1]\n",
    "    except:\n",
    "        pass\n",
    "    if CITY != None:\n",
    "        CITY = CITY.capitalize()\n",
    "        CITTA = True\n",
    "    \n",
    "    CITY2 = None\n",
    "    try: \n",
    "        if LOC[1][0] != None:\n",
    "            CITY2 = LOC[1][0]\n",
    "        if LOC[1][1] != None:\n",
    "            CITY2 = LOC[1][0] + \" \"+ LOC[1][1]\n",
    "    except:\n",
    "        pass\n",
    "    if CITY2 != None:\n",
    "        CITY2 = CITY2.capitalize()\n",
    "        CITTA2 = True\n",
    "        \n",
    "    #Capture if a person is given in the input\n",
    "    PERSON = None\n",
    "    try: \n",
    "        if PER[0][0] != None:\n",
    "            PERSON = PER[0][0]\n",
    "        if PER[0][1] != None:\n",
    "            PERSON = PER[0][0] + \" \"+ PER[0][1]\n",
    "    except:\n",
    "        pass\n",
    "    if PERSON != None:\n",
    "        person = True\n",
    "        \n",
    " \n",
    " #############################################################################################################################\n",
    "    # Language Detector\n",
    "    try:\n",
    "        detector = Detector(human).language.name\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Tokenize \n",
    "    human = human.split()\n",
    "    hum = []\n",
    "    \n",
    "    for tok in human:\n",
    "        hum.append(re.sub(r'[^\\w]', '', tok))\n",
    "    hum.append('EOS')\n",
    "    \n",
    "\n",
    "    for i in range (len(hum)-1):\n",
    "        # normalize the input\n",
    "        token = hum[i].lower()\n",
    "        next_token = hum[i+1].lower()\n",
    "       \n",
    "        # Assign the multilingual vectors to the input words\n",
    "        # assign a zero confidence to stopwords\n",
    "        vect, vect_sp, vect_it, vect_rus, vect_est = token2multi_vectors(token)\n",
    "        next_vect, next_vect_sp, next_vect_it, next_vect_rus, next_vect_est = token2multi_vectors(next_token)\n",
    "        languages = [vect, vect_est, vect_it, vect_rus, vect_sp]\n",
    "        \n",
    "         # capture if the time is provided in the input (today, tomorrow)\n",
    "        oggi = when_def(today_key, 0.5, languages)\n",
    "        domani = when_def(tomorrow_key, 0.5, languages)\n",
    "        ieri = when_def(yesterday_key, 0.5, languages)\n",
    "        if oggi > 0 and oggi > domani:\n",
    "            today = True\n",
    "        if domani > 0 and domani > oggi:\n",
    "            tomorrow = True\n",
    "        \n",
    "######## WEATHER BOT ##########################################################################################################\n",
    "\n",
    "        # \"tiempo\" and \"tempo\" have vectors similar to \"time\" rather than weather. We manually correct this.\n",
    "        #similarities_weather_sp['tiempo'] = np.array([[0.0]])\n",
    "        #similarities_weather_it['tempo'] = np.array([[0.0]])\n",
    "        \n",
    "        # UNCOMMENT THIS PART TO HAVE THE WEATHERBOT WORKING WT THE KEYWORD \"WEATHER\" IN THE OTHER LANGUAGES\n",
    "        \n",
    "        #if token == 'tiempo':\n",
    "         #   similarities_weather_sp['tiempo'] = np.array([[0.7]])\n",
    "        #elif token == 'tempo':\n",
    "         #   similarities_weather_it['tempo'] = np.array([[0.7]])\n",
    "       # elif token == 'Погода':\n",
    "        #    similarities_weather_ru['Погода'] = np.array([[0.7]])\n",
    "        \n",
    "        \n",
    "        # We compute the similarities between the multilingual vectors\n",
    "        for key, value in keywords_weather.items():\n",
    "            value = value.reshape(1,300)\n",
    "            \n",
    "            # simbot_en, simbot_sp, simbot_it, simbot_ru, simbot_est, multibot):\n",
    "            sorted_simil_weather = highest_sim_over_language(similarities_weather_en, similarities_weather_sp,\n",
    "                                    similarities_weather_it, similarities_weather_ru, similarities_weather_est, multi_weather)\n",
    "            \n",
    "        # Language identifier\n",
    "        language_weather = language_identifier(sorted_simil_weather)\n",
    "        \n",
    "        # get the highest key-value pair\n",
    "        if sorted_simil_weather[0][1] > 0.4:\n",
    "            tokens_weather_simil[sorted_simil_weather[0][0]] = sorted_simil_weather[0][1] \n",
    "           \n",
    "        # check if the highest key-value pair is over the threshold and append for the confidence\n",
    "        # CONFIDENCE IS GIVEN BY THE SUM OF CAPTURED KEYWORDS ABOVE THE THRESHOLD\n",
    "        \n",
    "        if sorted_simil_weather[0][1][0] > 0.46:\n",
    "            confidence_weather.append(sorted_simil_weather[0][1] +2.6)\n",
    "        \n",
    "         \n",
    " \n",
    " ##### TRAVEL BOT ############################################################################################################\n",
    "\n",
    "        for key, value in keywords_travel.items():\n",
    "            value = value.reshape(1,300)\n",
    "            \n",
    "            # simbot_en, simbot_sp, simbot_it, simbot_ru, simbot_est, multibot):\n",
    "            sorted_simil_travel = highest_sim_over_language(similarities_travel_en, similarities_travel_sp,\n",
    "                                    similarities_travel_it, similarities_travel_ru, similarities_travel_est, multi_travel)\n",
    "            \n",
    "        # Language identifier\n",
    "        language_travel = language_identifier(sorted_simil_travel)\n",
    "        \n",
    "        # get the highest key-value pair\n",
    "        if sorted_simil_travel[0][1] > 0.4:\n",
    "            tokens_travel_simil[sorted_simil_travel[0][0]] = sorted_simil_travel[0][1] \n",
    "           \n",
    "        # check if the highest key-value pair is over the threshold and append for the confidence\n",
    "        # CONFIDENCE IS GIVEN BY THE SUM OF CAPTURED KEYWORDS ABOVE THE THRESHOLD\n",
    "        \n",
    "        if sorted_simil_travel[0][1][0] > 0.46:\n",
    "            confidence_travel.append(sorted_simil_travel[0][1] +2.6)\n",
    "        \n",
    "        # if we find that FROM and 1st CITY encountered is above the threshold, we set it as the starting point\n",
    "        # N.B. Unfortunately we cannot make it more general (i.e. from + ANY CITY) bcs the cos_sim of prepositions\n",
    "        # TO in other languages are higher than the FROM ones (in his case then, they will always be detected as arriving point)\n",
    "        # but in others as russian FROM is higher than TO\n",
    "        if CITY2 != None:\n",
    "            partenza = travel_bigram('from', CITY, 0.35)\n",
    "            if partenza != None:\n",
    "                leaving = partenza\n",
    "            if cosine_similarity(table_eng['from'].reshape(1,300), vect) > 0.8 and next_token.capitalize() == CITY:\n",
    "                leaving = CITY\n",
    "                \n",
    "            \n",
    "            \n",
    "            arrivo = travel_bigram('to', CITY2, 0.45)\n",
    "            if arrivo != None:\n",
    "                arriving = arrivo\n",
    "            if cosine_similarity(table_eng['to'].reshape(1,300), vect) > 0.8 and next_token.capitalize() == CITY2:\n",
    "                arriving = CITY2\n",
    "            \n",
    "            \n",
    "            \n",
    "            if arriving == None:\n",
    "                arrivo2= travel_bigram('to', CITY, 0.45) \n",
    "                if arrivo2 !=None:\n",
    "                    arriving = arrivo2\n",
    "                    leaving = CITY2\n",
    "                if cosine_similarity(table_eng['to'].reshape(1,300), vect) > 0.8 and next_token.capitalize() == CITY:\n",
    "                    arriving = CITY\n",
    "                    leaving = CITY2\n",
    "            \n",
    "            \n",
    "            if leaving == None: \n",
    "                partenza2 = travel_bigram('from', CITY2, 0.35)\n",
    "                if partenza2 != None:\n",
    "                    leaving = partenza2\n",
    "                if cosine_similarity(table_eng['from'].reshape(1,300), vect) > 0.8 and next_token.capitalize() == CITY:\n",
    "                    leaving = CITY2\n",
    "                    arriving = CITY\n",
    "     \n",
    "        if CITY != None and CITY2 == None:\n",
    "            partenza = travel_bigram('from', CITY, 0.35)\n",
    "            if partenza != None:\n",
    "                leaving = partenza\n",
    "            if cosine_similarity(table_eng['from'].reshape(1,300), vect) > 0.8 and next_token.capitalize() == CITY:\n",
    "                leaving = CITY\n",
    "            \n",
    "            arrivo = travel_bigram('to', CITY, 0.45)\n",
    "            if arrivo != None:\n",
    "                arriving = arrivo\n",
    "            if cosine_similarity(table_eng['to'].reshape(1,300), vect) > 0.8 and next_token.capitalize() == CITY:\n",
    "                arriving = CITY\n",
    "            \n",
    "        if cosine_similarity(table_eng['going'].reshape(1, 300), vect) == 1:\n",
    "            confidence_travel.append(-13)\n",
    "            \n",
    "        \n",
    "########## WIKI BOT #########################################################################################################\n",
    "        \n",
    "        for key, value in keywords_wikipedia.items():\n",
    "            value = value.reshape(1,300)\n",
    "            \n",
    "            # simbot_en, simbot_sp, simbot_it, simbot_ru, simbot_est, multibot):\n",
    "            sorted_simil_wiki = highest_sim_over_language(similarities_wiki_en, similarities_wiki_sp,\n",
    "                                    similarities_wiki_it, similarities_wiki_ru, similarities_wiki_est, multi_wiki)\n",
    "        # Language identifier\n",
    "        language = language_identifier(sorted_simil_wiki)\n",
    "            \n",
    "        # get the highest key-value pair\n",
    "        if sorted_simil_wiki[0][1] > 0.4:\n",
    "            tokens_wiki_simil[sorted_simil_wiki[0][0]] = sorted_simil_wiki[0][1]\n",
    "        \n",
    "        # check if the highest key-value pair is over the threshold and append for the confidence\n",
    "        # CONFIDENCE IS GIVEN BY THE SUM OF CAPTURED KEYWORDS ABOVE THE THRESHOLD\n",
    "        if sorted_simil_wiki[0][1][0] > 0.4:\n",
    "            confidence_wiki.append(sorted_simil_wiki[0][1])\n",
    "   \n",
    "        if cosine_similarity(table_eng['who'].reshape(1,300), vect) + cosine_similarity(table_eng['is'].reshape(1,300), next_vect) > 1.9:\n",
    "                confidence_wiki.append(1)\n",
    "        wiki_bigram('who', 1)\n",
    "        \n",
    "        if cosine_similarity(table_eng['what'].reshape(1,300), vect) + cosine_similarity(table_eng['is'].reshape(1,300), next_vect) > 1.9:\n",
    "                confidence_wiki.append(2.6)\n",
    "                topic = hum[i+2:-1]\n",
    "                topic= \" \".join(topic)\n",
    "                WHAT = True\n",
    "        what = wiki_bigram('what', 1)\n",
    "        if what == True:\n",
    "            topic = hum[i+2:-1]\n",
    "            topic= \" \".join(topic)\n",
    "            WHAT = True\n",
    "            \n",
    "        if cosine_similarity(table_eng['where'].reshape(1,300), vect) + cosine_similarity(table_eng['is'].reshape(1,300), next_vect) > 1.9:\n",
    "                confidence_wiki.append(1)    \n",
    "        wiki_bigram('where', 1)\n",
    "    \n",
    "######## TIME BOT ############################################################################################################\n",
    "        for key, value in keywords_time.items():\n",
    "            value = value.reshape(1,300)\n",
    "            \n",
    "            # simbot_en, simbot_sp, simbot_it, simbot_ru, simbot_est, multibot):\n",
    "            sorted_simil_time = highest_sim_over_language(similarities_time_en, similarities_time_sp,\n",
    "                                    similarities_time_it, similarities_time_ru, similarities_time_est, multi_time)\n",
    "            \n",
    "        # Language identifier\n",
    "        language_time = language_identifier(sorted_simil_time)\n",
    "           \n",
    "        # check if the highest key-value pair is over the threshold and append for the confidence\n",
    "        # CONFIDENCE IS GIVEN BY THE SUM OF CAPTURED KEYWORDS ABOVE THE THRESHOLD\n",
    "        if sorted_simil_time[0][1][0] > 0.65:\n",
    "            confidence_time.append(sorted_simil_time[0][1] +2.6)\n",
    "        \n",
    "        \n",
    "        if cosine_similarity(table_eng['what'].reshape(1,300), vect) + cosine_similarity(table_eng['time'].reshape(1,300), next_vect) > 1.9:\n",
    "                confidence_time.append(9)    \n",
    "        \n",
    "        languages = [vect_est, vect_it, vect_rus, vect_sp]\n",
    "        languages_next = [next_vect_est, next_vect_it, next_vect_rus, next_vect_sp] \n",
    "        for i in range(len(languages)):\n",
    "            sim1 = cosine_similarity(table_eng['what'].reshape(1,300), languages[i])\n",
    "            sim2 = cosine_similarity(table_eng['hours'].reshape(1,300), languages_next[i])\n",
    "            if  sim1 +  sim2 > 0.75:\n",
    "                confidence_time.append(5)\n",
    "        \n",
    "    \n",
    "######## CONFIDENCE OVERALL ###############################################################################################################\n",
    "\n",
    "    # Compute the confidences and we select the bot with the highest confidence\n",
    "    conf_weather = sum(confidence_weather)\n",
    "    conf_wiki = sum(confidence_wiki)\n",
    "    conf_travel=sum(confidence_travel)\n",
    "    conf_time = sum(confidence_time)\n",
    "    if person == True:\n",
    "        conf_wiki = conf_wiki + 3\n",
    "    if today or tomorrow or yesterday == True:\n",
    "        conf_wiki = conf_wiki - 4\n",
    "    if CITY2 == True:\n",
    "        conf_travel += 3\n",
    "    confidence_overall = sorted([conf_weather, conf_travel, conf_wiki, conf_time], reverse=True)\n",
    "    \n",
    "######## WEATHER RESPONSE #################################################################################################### \n",
    "    if confidence_overall[0] == conf_weather and confidence_overall[0] > 0:\n",
    "        \n",
    "        # we use Polyglot only if there is a conflict between IT and SP and we give precedence to Polyglot\n",
    "        if detector == 'Spanish' and language == 'IT':\n",
    "            language_weather = 'SP'\n",
    "        elif detector == 'Italian' and language == 'SP':\n",
    "            language_weather = 'IT'\n",
    "        \n",
    "        # this is to get a different answer according to the keyword, in this case all keywords will give a same answer\n",
    "        #response = sorted(tokens_weather_simil.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        \n",
    "        # We hard-coded dummy answers for all the languages, they provide different answers whether locality and time are given\n",
    "        # in the input: if they are not given, the function will ask for it.\n",
    "        if language_weather == 'EN':\n",
    "            \n",
    "            #phrase_locality, phrase_when, asnwer_1, answer_2, TODAY, TOMORROW, sorry_answer\n",
    "            temp= answer_weather(today, tomorrow, \"BOT: In which location?\", \"When?\", \"today\", \"tomorrow\",CITY,\n",
    "                           \"I am sorry, we don't have information about this location\", 'en')\n",
    "        \n",
    "        elif language_weather == 'SP':\n",
    "            try:\n",
    "                CITY = \"_\".join(CITY.split())\n",
    "                website = 'https://es.wikipedia.org/wiki/'+CITY\n",
    "                CITY = selenium (website)\n",
    "                answer_weather(today,tomorrow, \"En cual ciudad?\", \"Cuando?\",  \"hoy\", \"mañana\", CITY,\n",
    "                                      \"Lo siento, no tenemos información sobre esta ciudad\", 'es')\n",
    "            except:\n",
    "                if CITY != None:\n",
    "                    CITY = translator.translate(CITY, dest='en').text\n",
    "                    answer_weather(today,tomorrow,  \"En cual ciudad?\", \"Cuando?\",  \"hoy\", \"mañana\", CITY,\n",
    "                                      \"Lo siento, no tenemos información sobre esta ciudad\", 'es')\n",
    "                    \n",
    "                else:\n",
    "                    answer_weather(today,tomorrow,  \"En cual ciudad?\", \"Cuando?\",  \"hoy\", \"mañana\", CITY,\n",
    "                                      \"Lo siento, no tenemos información sobre esta ciudad\", 'es')\n",
    "                \n",
    "        elif language_weather == 'IT':\n",
    "            try:\n",
    "                CITY = \"_\".join(CITY.split())\n",
    "                website = 'https://it.wikipedia.org/wiki/'+CITY\n",
    "                CITY = selenium (website)\n",
    "                answer_weather(today,tomorrow,\"In quale citta'?\", 'Quando?', \"oggi\", \"domani\", CITY,\n",
    "                           \"Mi dispiace, non disponiamo di informazioni riguardo a questa citta'\", 'it')   \n",
    "            except:\n",
    "                if CITY != None:\n",
    "                    CITY = translator.translate(CITY, dest='en').text\n",
    "                    answer_weather(today,tomorrow, \"In quale citta'?\", 'Quando?', \"oggi\", \"domani\", CITY,\n",
    "                           \"Mi dispiace, non disponiamo di informazioni riguardo a questa citta'\", \"it\")\n",
    "                else:\n",
    "                    answer_weather(today,tomorrow, \"In quale citta'?\", 'Quando?', \"oggi\", \"domani\", CITY,\n",
    "                           \"Mi dispiace, non disponiamo di informazioni riguardo a questa citta'\", 'it')\n",
    "                \n",
    "        elif language_weather == 'RU':\n",
    "            try:\n",
    "                CITY = \"_\".join(CITY.split())\n",
    "                website = 'https://ru.wikipedia.org/wiki/'+CITY\n",
    "                CITY = selenium (website)\n",
    "                answer_weather(today,tomorrow,  \"в каком городе?\", 'когда?', \"сегодня\", \"завтра\", CITY,\n",
    "                               \"Извините, у нас нет информации об этом городе\", 'ru')   \n",
    "            except:\n",
    "                if CITY != None:\n",
    "                    CITY = translator.translate(CITY, dest='en').text \n",
    "                    answer_weather(today,tomorrow, \"в каком городе?\", 'когда?', \"сегодня\", \"завтра\", CITY,\n",
    "                                   \"Извините, у нас нет информации об этом городе\", 'ru')    \n",
    "                else:\n",
    "                    answer_weather(today,tomorrow,\"в каком городе?\", 'когда?',  \"сегодня\", \"завтра\", CITY,\n",
    "                                   \"Извините, у нас нет информации об этом городе\", 'ru')  \n",
    "                    \n",
    "        elif language_weather == 'EST':\n",
    "            try:\n",
    "                CITY = \"_\".join(CITY.split())\n",
    "                website = 'https://et.wikipedia.org/wiki/'+CITY\n",
    "                CITY = selenium (website)\n",
    "                answer_weather(today,tomorrow,  \"Millises linnas?\", 'Kui', \"täna\", \"homme\", CITY,\n",
    "                           \"Vabandust, meil ei ole selle linna kohta teavet\", 'et')   \n",
    "            except:\n",
    "                if CITY != None:\n",
    "                    CITY = translator.translate(CITY, dest='en').text\n",
    "                    answer_weather(today,tomorrow, \"Millises linnas?\", 'Kui?', \"täna\", \"homme\", CITY,\n",
    "                           \"Vabandust, meil ei ole selle linna kohta teavet\", 'et')  \n",
    "                else:\n",
    "                    answer_weather(today,tomorrow,  \"Millises linnas?\", 'Kui?',\"täna\", \"homme\", CITY,\n",
    "                           \"Vabandust, meil ei ole selle linna kohta teavet\", 'et')  \n",
    "                    \n",
    "####### TRAVEL ANSWER ######################################################################################################\n",
    "    elif confidence_overall[0] == conf_travel and confidence_overall[0] > 0:\n",
    "        # we use Polyglot only if there is a conflict between IT and SP and we give precedence to Polyglot\n",
    "        if detector == 'Spanish' and language_travel == 'IT':\n",
    "            language_travel = 'SP'\n",
    "        elif detector == 'Italian' and language_travel == 'SP':\n",
    "            language_travel = 'IT'\n",
    "        \n",
    "        if language_travel == 'EN':\n",
    "            answer_travel(CITY, CITY2, leaving, arriving, today, tomorrow, 'en', \"BOT: Which is your destination?\", \n",
    "            \"BOT: From which city would you like to leave?\", \"BOT: I am sorry, the city you inserted doesn't coincide with the one you told me earlier\",\n",
    "            \"BOT: When?\", \"today\", \"tomorrow\")\n",
    "        elif language_travel == 'SP':\n",
    "            answer_travel(CITY, CITY2, leaving, arriving, today, tomorrow, 'es', \"BOT: ¿Cuál es tu destino?\",\n",
    "            \"BOT: ¿De qué ciudad te gustaría irte?\", \"BOT: Lo siento, la ciudad insertada no coincide con la que me dijiste antes\",\n",
    "             \"BOT: Cuando?\", \"hoy\", \"mañana\")\n",
    "        elif language_travel == 'IT':\n",
    "            answer_travel(CITY, CITY2, leaving, arriving, today, tomorrow, 'it', \"BOT: qual è la tua destinazione?\", \n",
    "             \"BOT: Da quale città ti piacerebbe partire?\", \"BOT: Mi dispiace, la città che hai digitato non corrisponde a quella inserita in precedenza\",\n",
    "            \"BOT: Quando?\", \"oggi\", \"domani\")\n",
    "        elif language_travel == 'EST':\n",
    "            answer_travel(CITY, CITY2, leaving, arriving, today, tomorrow, 'et', \"BOT: mis on teie sihtkoht?\",\n",
    "            \"BOT: milline linn sooviksite lahkuda?\", \"BOT: Mul on kahju, et sisestatud linn ei lange kokku sellega, mida sa varem ütlesid\",\n",
    "             \"BOT: Millal?\", \"täna\", \"homme\")\n",
    "        elif language_travel == 'RU':\n",
    "            answer_travel(CITY, CITY2, leaving, arriving, today, tomorrow, 'ru', \"BOT: Куда ты направляешься\",\n",
    "            \"BОТ: из какого города вы хотели бы уехать?\", \"БОТ: извините, введенный вами город не совпадает с тем, который вы мне сказали ранее\",\n",
    "            \"BОТ: Когда?\", \"Сегодня\", \"Завтра\")      \n",
    "            \n",
    "        \n",
    "\n",
    "######## WIKIPEDIA ANSWER ######################################################################################################\n",
    "    elif confidence_overall[0] == conf_wiki and confidence_overall[0] > 0:\n",
    "        \n",
    "        if detector == 'Spanish' and language == 'IT':\n",
    "            language = 'SP'\n",
    "        elif detector == 'Italian' and language == 'SP':\n",
    "            language = 'IT'\n",
    "            \n",
    "        # this is to get a different answer according to the keyword, in this case all keywords will give a same answer\n",
    "        #response = sorted(tokens_wiki_simil.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        \n",
    "        # We hard-coded dummy answers for all the languages, they provide different answers whether locality and time are given\n",
    "        # in the input: if they are not given, the function will ask for it.\n",
    "      \n",
    "        \n",
    "        if language == 'EN':\n",
    "            wikipedia.set_lang(\"en\")\n",
    "            answer_wiki(\"BOT: Sorry I am not sure I understood, which topic were you looking for?\",\n",
    "                        \"BOT: Sorry we don't have information about\")\n",
    "\n",
    "        elif language == 'SP':\n",
    "            wikipedia.set_lang(\"es\")\n",
    "            answer_wiki(\"BOT: Lo siento, no estoy seguro de haber entendido, ¿qué tema buscabas?\",\n",
    "                       \"Lo sentimos, no tenemos información sobre\")\n",
    "            \n",
    "        elif language == 'RU':\n",
    "            wikipedia.set_lang(\"ru\")\n",
    "            answer_wiki(\"Извините, я не уверен, что понял, какую тему вы искали?\",\n",
    "                       \"Извините, у нас нет информации о\")\n",
    "        \n",
    "        elif language == 'EST':\n",
    "            wikipedia.set_lang(\"et\")\n",
    "            answer_wiki(\"Vabandust, et ma ei ole kindel, et sain aru, millist teemat otsisite?\",\n",
    "                       \"Vabandame, et meil pole teavet\")\n",
    "         \n",
    "        elif language == 'IT':\n",
    "            wikipedia.set_lang(\"it\")\n",
    "            answer_wiki(\"Mi dispiace non sono sicuro di aver capito, quale argomento stavi cercando?\",\n",
    "                       \"Ci dispiace, non abbiamo informazioni su\")\n",
    "        \n",
    "##### TIME ANSWER ############################################################################################################\n",
    "    elif confidence_overall[0] == conf_time and confidence_overall[0] > 0:\n",
    "        if detector == 'Spanish' and language == 'IT':\n",
    "            language = 'SP'\n",
    "        elif detector == 'Italian' and language == 'SP':\n",
    "            language = 'IT'\n",
    "        \n",
    "        if language == 'EN':\n",
    "            print(\"BOT: It's\", datetime.now().strftime(\"%H:%M\"))\n",
    "            \n",
    "        elif language == 'SP':\n",
    "            print (\"BOT: Son las\", datetime.now().strftime(\"%H:%M\"))\n",
    "            \n",
    "        elif language == 'RU':\n",
    "            print(\"BOT: Это\", datetime.now().strftime(\"%H:%M\"), \"вечера\")\n",
    "           \n",
    "        elif language == 'EST':\n",
    "            print(\"BOT: See on\", datetime.now().strftime(\"%H:%M\"))\n",
    "           \n",
    "        elif language == 'IT':\n",
    "            print(\"BOT: Sono le\", datetime.now().strftime(\"%H:%M\"))\n",
    "        \n",
    "        \n",
    "    # in case the threshold is not reached\n",
    "    else:\n",
    "        print(\"Sorry I didn't understand, could you repeat in other words?\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
